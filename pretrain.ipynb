{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d9b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dff356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./0121-政府风险偏好-关键词.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2beb2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>词汇</th>\n",
       "      <th>词性</th>\n",
       "      <th>举例</th>\n",
       "      <th>处理</th>\n",
       "      <th>备注</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>实义词</td>\n",
       "      <td>消极</td>\n",
       "      <td>困难、问题、不足、难点、难题、风险、压力、矛盾、危机、瓶颈、制约、短板</td>\n",
       "      <td>只要提到这些消极词汇，均认为这句话是【保守】的，打标签“0”</td>\n",
       "      <td>优先级最高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>中性或积极</td>\n",
       "      <td>其他</td>\n",
       "      <td>需要和程度副词以及其他词汇搭配判断</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>程度副词</td>\n",
       "      <td>高度</td>\n",
       "      <td>非常、很、十分、最、格外、相当、极其、显著、大幅、重大、明显、高位</td>\n",
       "      <td>只要提到这些词汇，均认为这句话是【激进】的，打标签“1”</td>\n",
       "      <td>如果出现矛盾，可标注，后续人工进行判断</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>轻微</td>\n",
       "      <td>不完全、相对、稍、还、仍、偏、较好</td>\n",
       "      <td>只要提到这些词汇，均认为这句话是【保守】的，打标签“0”</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>其他</td>\n",
       "      <td>积极</td>\n",
       "      <td>大刀阔斧、雷厉风行、毫不动摇、坚持不懈、强势、全力、发力、力促、力争、争创、大力、着力、倾力...</td>\n",
       "      <td>只要提到这些词汇，均认为这句话是【激进】的，打标签“1”</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>消极</td>\n",
       "      <td>紧日子、紧张、不易、艰难、不寻常、不平凡、下行压力、严峻、不稳定、不确定、不充分、不快、不够...</td>\n",
       "      <td>只要提到这些词汇，均认为这句话是【保守】的，打标签“0”</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     词汇     词性                                                 举例  \\\n",
       "0   实义词     消极                困难、问题、不足、难点、难题、风险、压力、矛盾、危机、瓶颈、制约、短板   \n",
       "1   NaN  中性或积极                                                 其他   \n",
       "2  程度副词     高度                  非常、很、十分、最、格外、相当、极其、显著、大幅、重大、明显、高位   \n",
       "3   NaN     轻微                                  不完全、相对、稍、还、仍、偏、较好   \n",
       "4    其他     积极  大刀阔斧、雷厉风行、毫不动摇、坚持不懈、强势、全力、发力、力促、力争、争创、大力、着力、倾力...   \n",
       "5   NaN     消极  紧日子、紧张、不易、艰难、不寻常、不平凡、下行压力、严峻、不稳定、不确定、不充分、不快、不够...   \n",
       "\n",
       "                               处理                   备注  \n",
       "0  只要提到这些消极词汇，均认为这句话是【保守】的，打标签“0”                优先级最高  \n",
       "1               需要和程度副词以及其他词汇搭配判断                  NaN  \n",
       "2    只要提到这些词汇，均认为这句话是【激进】的，打标签“1”  如果出现矛盾，可标注，后续人工进行判断  \n",
       "3    只要提到这些词汇，均认为这句话是【保守】的，打标签“0”                  NaN  \n",
       "4    只要提到这些词汇，均认为这句话是【激进】的，打标签“1”                  NaN  \n",
       "5    只要提到这些词汇，均认为这句话是【保守】的，打标签“0”                  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fcdf97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_up = []\n",
    "keywords_down = []\n",
    "for index,row in df.iterrows():\n",
    "    if '1' in row['处理']:\n",
    "        for word in row['举例'].split('、'):\n",
    "            keywords_up.append(word.replace('（','').replace('）',''))\n",
    "    elif '0' in row['处理']:\n",
    "        keywords_down += row['举例'].split('、')\n",
    "keywords_up.append('更')\n",
    "keywords_up.append('越')\n",
    "keywords_up.append('快速')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf3fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import os\n",
    "import re\n",
    "import pdb\n",
    "path1 = './地级工作报告(2007-2021)' #待读取文件的文件夹绝对地址\n",
    "files = os.listdir(path1) #获得文件夹中所有文件的名称列表\n",
    "result = []\n",
    "test_data = []\n",
    "for file in files:\n",
    "    try:\n",
    "        if int(file[-9:-5]) >= 2010 and int(file[-9:-5]) <= 2013:\n",
    "            path = path1 +  '/' + file\n",
    "            document = Document(path)\n",
    "            sentences = document.paragraphs[0].text.strip().replace('\\n','').split('。')\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                keywords_extract = []\n",
    "                keywords_extract = []\n",
    "                labels = []\n",
    "                words_up = re.findall('|'.join(keywords_up),sentence)\n",
    "                words_down = re.findall('|'.join(keywords_down),sentence)\n",
    "                labels.append(str(len(words_up)))\n",
    "                labels.append(str(len(words_down)))\n",
    "                result.append([sentence,','.join(labels)])\n",
    "        else:\n",
    "            path = path1 +  '/' + file\n",
    "            document = Document(path)\n",
    "            sentences = document.paragraphs[0].text.strip().replace('\\n','').split('。')\n",
    "            for sentence in sentences:\n",
    "                keywords_extract = []\n",
    "                keywords_extract = []\n",
    "                labels = []\n",
    "                words_up = re.findall('|'.join(keywords_up),sentence)\n",
    "                words_down = re.findall('|'.join(keywords_down),sentence)\n",
    "                labels.append(str(len(words_up)))\n",
    "                labels.append(str(len(words_down)))\n",
    "                test_data.append([sentence,','.join(labels)])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162b4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(result,columns=['句子','标签'])\n",
    "results = results.drop_duplicates().reset_index(drop=True)\n",
    "train_result = result[ :int(0.9 * len(result))]\n",
    "dev_result = result[int(0.9 * len(result)):]\n",
    "train_result_df = pd.DataFrame(train_result,columns=['句子','标签'])\n",
    "dev_result_df = pd.DataFrame(dev_result,columns=['句子','标签'])\n",
    "test_data_df = pd.DataFrame(test_data,columns=['句子','标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a545e65a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data_dir = './data'\n",
    "def preprocess_data(data_type):\n",
    "    train_data_process = []\n",
    "    dev_data_process = []\n",
    "    test_data_process = []\n",
    "    train_data_path =  data_dir + '/train_' + data_type + '.txt'\n",
    "    dev_data_path = data_dir + '/dev_' + data_type + '.txt'\n",
    "    test_data_path =  data_dir + '/test_' + data_type + '.txt'\n",
    "    if data_type == 'model_1':\n",
    "        for index,row in train_result_df.iterrows():\n",
    "            if '0' not in row['标签'].split(','):\n",
    "                continue\n",
    "            if row['标签'].split(',')[0] == '0':\n",
    "                train_data_process.append([row['句子'],'0'])\n",
    "            else:\n",
    "                train_data_process.append([row['句子'],'1'])\n",
    "        for index,row in dev_result_df.iterrows():\n",
    "            if '0' not in row['标签'].split(','):\n",
    "                continue\n",
    "            if row['标签'].split(',')[0] == '0':\n",
    "                dev_data_process.append([row['句子'],'0'])\n",
    "            else:\n",
    "                dev_data_process.append([row['句子'],'1'])\n",
    "        for index,row in test_data_df.iterrows():\n",
    "            if '0' not in row['标签'].split(','):\n",
    "                continue\n",
    "            if row['标签'].split(',')[0] == '0':\n",
    "#                 pdb.set_trace()\n",
    "                test_data_process.append([row['句子'],'0'])\n",
    "            else:\n",
    "                test_data_process.append([row['句子'],'1'])\n",
    "        train_data_df = pd.DataFrame(train_data_process,columns=['sentence','label']).drop_duplicates().reset_index(drop=True)\n",
    "        train_data_df_1 = train_data_df[train_data_df['label']=='1']\n",
    "        train_data_df_0 = train_data_df[train_data_df['label']=='0']\n",
    "        print('model_1,标签1:{},标签0:{}'.format(len(train_data_df_1),len(train_data_df_0)))\n",
    "    if data_type == 'model_2':\n",
    "        for index,row in train_result_df.iterrows():\n",
    "            if row['标签'].split(',')[0] > row['标签'].split(',')[1] and row['标签'].split(',')[1] == '0':\n",
    "                train_data_process.append([row['句子'],'1'])\n",
    "            else:\n",
    "                train_data_process.append([row['句子'],'0'])\n",
    "        for index,row in dev_result_df.iterrows():\n",
    "            if row['标签'].split(',')[0] > row['标签'].split(',')[1] and row['标签'].split(',')[1] == '0':\n",
    "                dev_data_process.append([row['句子'],'1'])\n",
    "            else:\n",
    "                dev_data_process.append([row['句子'],'0'])\n",
    "        for index,row in test_data_df.iterrows():\n",
    "            if row['标签'].split(',')[0] > row['标签'].split(',')[1] and row['标签'].split(',')[1] == '0':\n",
    "                test_data_process.append([row['句子'],'1'])\n",
    "            else:\n",
    "                test_data_process.append([row['句子'],'0'])\n",
    "        train_data_df = pd.DataFrame(train_data_process,columns=['sentence','label']).drop_duplicates().reset_index(drop=True)\n",
    "        train_data_df_1 = train_data_df[train_data_df['label']=='1']\n",
    "        train_data_df_0 = train_data_df[train_data_df['label']=='0']\n",
    "        print('model_2,标签1:{},标签0:{}'.format(len(train_data_df_1),len(train_data_df_0)))\n",
    "    if data_type == 'model_3':\n",
    "        for index,row in train_result_df.iterrows():\n",
    "            if '0' in row['标签'].split(',') and len(set(row['标签'].split(','))) == 2:\n",
    "                if row['标签'].split(',')[0] > row['标签'].split(',')[1]:\n",
    "                    train_data_process.append([row['句子'],'1'])\n",
    "                else:\n",
    "                    train_data_process.append([row['句子'],'0'])\n",
    "            else:\n",
    "                train_data_process.append([row['句子'],'2'])\n",
    "        for index,row in dev_result_df.iterrows():\n",
    "            if '0' in row['标签'].split(',') and len(set(row['标签'].split(','))) == 2:\n",
    "                if row['标签'].split(',')[0] > row['标签'].split(',')[1]:\n",
    "                    dev_data_process.append([row['句子'],'1'])\n",
    "                else:\n",
    "                    dev_data_process.append([row['句子'],'0'])\n",
    "            else:\n",
    "                dev_data_process.append([row['句子'],'2'])\n",
    "        for index,row in test_data_df.iterrows():\n",
    "            if '0' in row['标签'].split(',') and len(set(row['标签'].split(','))) == 2:\n",
    "                if row['标签'].split(',')[0] > row['标签'].split(',')[1]:\n",
    "                    test_data_process.append([row['句子'],'1'])\n",
    "                else:\n",
    "                    test_data_process.append([row['句子'],'0'])\n",
    "            else:\n",
    "                test_data_process.append([row['句子'],'2'])\n",
    "        train_data_df = pd.DataFrame(train_data_process,columns=['sentence','label']).drop_duplicates().reset_index(drop=True)\n",
    "        train_data_df_0 = train_data_df[train_data_df['label']=='0']\n",
    "        train_data_df_1 = train_data_df[train_data_df['label']=='1']        \n",
    "        train_data_df_2 = train_data_df[train_data_df['label']=='2']\n",
    "        print('model_3, 标签2:{},标签1:{},标签0:{}'.format(len(train_data_df_2),len(train_data_df_1),len(train_data_df_0)))\n",
    "    if data_type == 'model_4':\n",
    "        for index,row in train_result_df.iterrows():\n",
    "            if '0' in row['标签'].split(','):\n",
    "                if len(set(row['标签'].split(','))) == 2:\n",
    "                    if row['标签'].split(',')[1]  != '0':\n",
    "                        train_data_process.append([row['句子'],'0'])\n",
    "                    else:\n",
    "                        train_data_process.append([row['句子'],'1'])\n",
    "                else:\n",
    "                    train_data_process.append([row['句子'],'0'])\n",
    "            else:\n",
    "                if row['标签'].split(',')[0] < row['标签'].split(',')[1]:\n",
    "                    train_data_process.append([row['句子'],'0'])\n",
    "                elif row['标签'].split(',')[0] > row['标签'].split(',')[1]:\n",
    "                    train_data_process.append([row['句子'],'1'])  \n",
    "                else:\n",
    "                    train_data_process.append([row['句子'],'0'])\n",
    "        for index,row in dev_result_df.iterrows():\n",
    "            if '0' in row['标签'].split(','):\n",
    "                if len(set(row['标签'].split(','))) == 2:\n",
    "                    if row['标签'].split(',')[1]  != '0':\n",
    "                        dev_data_process.append([row['句子'],'0'])\n",
    "                    else:\n",
    "                        dev_data_process.append([row['句子'],'1'])\n",
    "                else:\n",
    "                    dev_data_process.append([row['句子'],'0'])\n",
    "            else:\n",
    "                if row['标签'].split(',')[0] < row['标签'].split(',')[1]:\n",
    "                    dev_data_process.append([row['句子'],'0'])\n",
    "                elif row['标签'].split(',')[0] > row['标签'].split(',')[1]:\n",
    "                    dev_data_process.append([row['句子'],'1'])  \n",
    "                else:\n",
    "                    dev_data_process.append([row['句子'],'0'])\n",
    "        for index,row in test_data_df.iterrows():\n",
    "            if '0' in row['标签'].split(','):\n",
    "                if len(set(row['标签'].split(','))) == 2:\n",
    "                    if row['标签'].split(',')[1]  != '0':\n",
    "                        test_data_process.append([row['句子'],'0'])\n",
    "                    else:\n",
    "                        test_data_process.append([row['句子'],'1'])\n",
    "                else:\n",
    "                    test_data_process.append([row['句子'],'0'])\n",
    "            else:\n",
    "                if row['标签'].split(',')[0] < row['标签'].split(',')[1]:\n",
    "                    test_data_process.append([row['句子'],'0'])\n",
    "                elif row['标签'].split(',')[0] > row['标签'].split(',')[1]:\n",
    "                    test_data_process.append([row['句子'],'1'])  \n",
    "                else:\n",
    "                    test_data_process.append([row['句子'],'0'])\n",
    "        train_data_df = pd.DataFrame(train_data_process,columns=['sentence','label']).drop_duplicates().reset_index(drop=True)\n",
    "        train_data_df_0 = train_data_df[train_data_df['label']=='0']\n",
    "        train_data_df_1 = train_data_df[train_data_df['label']=='1']\n",
    "        train_data_df_2 = train_data_df[train_data_df['label']=='2']\n",
    "        print('model_4, 标签2：{}，标签1:{},标签0:{}'.format(len(train_data_df_2),len(train_data_df_1),len(train_data_df_0)))\n",
    "    if data_type == 'model_5': \n",
    "        for index,row in train_result_df.iterrows():\n",
    "            if '0' in row['标签'].split(','):\n",
    "                if len(set(row['标签'].split(','))) == 2:\n",
    "                    if row['标签'].split(',')[1]  != '0':\n",
    "                        train_data_process.append([row['句子'],'0'])\n",
    "                    else:\n",
    "                        train_data_process.append([row['句子'],'1'])\n",
    "                else:\n",
    "                    train_data_process.append([row['句子'],'2'])\n",
    "            else:\n",
    "                if row['标签'].split(',')[0] < row['标签'].split(',')[1]:\n",
    "                    train_data_process.append([row['句子'],'0'])\n",
    "                elif row['标签'].split(',')[0] > row['标签'].split(',')[1]:\n",
    "                    train_data_process.append([row['句子'],'1'])  \n",
    "                else:\n",
    "                    train_data_process.append([row['句子'],'2'])\n",
    "        for index,row in dev_result_df.iterrows():\n",
    "            if '0' in row['标签'].split(','):\n",
    "                if len(set(row['标签'].split(','))) == 2:\n",
    "                    if row['标签'].split(',')[1]  != '0':\n",
    "                        dev_data_process.append([row['句子'],'0'])\n",
    "                    else:\n",
    "                        dev_data_process.append([row['句子'],'1'])\n",
    "                else:\n",
    "                    dev_data_process.append([row['句子'],'2'])\n",
    "            else:\n",
    "                if row['标签'].split(',')[0] < row['标签'].split(',')[1]:\n",
    "                    dev_data_process.append([row['句子'],'0'])\n",
    "                elif row['标签'].split(',')[0] > row['标签'].split(',')[1]:\n",
    "                    dev_data_process.append([row['句子'],'1'])  \n",
    "                else:\n",
    "                    dev_data_process.append([row['句子'],'2'])\n",
    "        for index,row in test_data_df.iterrows():\n",
    "            if '0' in row['标签'].split(','):\n",
    "                if len(set(row['标签'].split(','))) == 2:\n",
    "                    if row['标签'].split(',')[1]  != '0':\n",
    "                        test_data_process.append([row['句子'],'0'])\n",
    "                    else:\n",
    "                        test_data_process.append([row['句子'],'1'])\n",
    "                else:\n",
    "                    test_data_process.append([row['句子'],'2'])\n",
    "            else:\n",
    "                if row['标签'].split(',')[0] < row['标签'].split(',')[1]:\n",
    "                    test_data_process.append([row['句子'],'0'])\n",
    "                elif row['标签'].split(',')[0] > row['标签'].split(',')[1]:\n",
    "                    test_data_process.append([row['句子'],'1'])  \n",
    "                else:\n",
    "                    test_data_process.append([row['句子'],'2'])\n",
    "        train_data_df = pd.DataFrame(train_data_process,columns=['sentence','label']).drop_duplicates().reset_index(drop=True)\n",
    "        train_data_df_0 = train_data_df[train_data_df['label']=='0']\n",
    "        train_data_df_1 = train_data_df[train_data_df['label']=='1']\n",
    "        train_data_df_2 = train_data_df[train_data_df['label']=='2']\n",
    "        print('model_5 标签2：{}，标签1:{},标签0:{}'.format(len(train_data_df_2),len(train_data_df_1),len(train_data_df_0)))\n",
    "    train_data_shuf = shuffle(train_data_df)\n",
    "    f_train = open(train_data_path,'w')\n",
    "    f_dev = open(dev_data_path,'w')\n",
    "    f_test = open(test_data_path,'w')\n",
    "    for index,row in train_data_shuf.iterrows():\n",
    "        f_train.write(row['sentence'] + '\\t' + row['label'] + '\\n')\n",
    "    for row in dev_data_process:\n",
    "        f_dev.write(row[0] + '\\t' + row[1] + '\\n') \n",
    "    for row in test_data_process:\n",
    "        f_test.write(row[0] + '\\t' + row[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20eb236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1,标签1:178150,标签0:139905\n",
      "model_2,标签1:178150,标签0:156443\n",
      "model_3, 标签2:146890,标签1:178150,标签0:9553\n",
      "model_4, 标签2：0，标签1:185440,标签0:149153\n",
      "model_5 标签2：136668，标签1:185440,标签0:12485\n"
     ]
    }
   ],
   "source": [
    "preprocess_data('model_1')\n",
    "preprocess_data('model_2')\n",
    "preprocess_data('model_3')\n",
    "preprocess_data('model_4')\n",
    "preprocess_data('model_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c6e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e51eb823",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(result,columns=['句子','标签\"1\"对应的关键词','标签“0”对应的关键词','标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "86086c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel('./安徽省安庆市2007标签处理结果.xlsx',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}