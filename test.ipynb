{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a0aee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heguxiang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(config.bert_path)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = x[0]  # 输入的句子\n",
    "        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "        _, pooled = self.bert(context, attention_mask=mask,return_dict=False)\n",
    "        out = self.fc(pooled)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf5dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "PAD, CLS = '[PAD]', '[CLS]' \n",
    "def build_dataset(config):\n",
    "    def load_dataset(path, pad_size=32):\n",
    "        contents = []\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                lin = line.strip()\n",
    "                if not lin:\n",
    "                    continue\n",
    "                if len(lin.split('\\t')) != 2:\n",
    "                    continue\n",
    "                content, label = lin.split('\\t')\n",
    "                token = config.tokenizer.tokenize(content)\n",
    "                token = [CLS] + token\n",
    "                seq_len = len(token)\n",
    "                mask = []\n",
    "                token_ids = config.tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "                if pad_size:\n",
    "                    if len(token) < pad_size:\n",
    "                        mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n",
    "                        token_ids += ([0] * (pad_size - len(token)))\n",
    "                    else:\n",
    "                        mask = [1] * pad_size\n",
    "                        token_ids = token_ids[:pad_size]\n",
    "                        seq_len = pad_size\n",
    "                contents.append((token_ids, int(label), seq_len, mask))\n",
    "        return contents\n",
    "    train = load_dataset(config.train_path, config.pad_size)\n",
    "    dev = load_dataset(config.dev_path, config.pad_size)\n",
    "    test = load_dataset(config.test_path, config.pad_size)\n",
    "    return train, dev, test\n",
    "class DatasetIterater(object):\n",
    "    def __init__(self, batches, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = batches\n",
    "        self.n_batches = len(batches) // batch_size\n",
    "        self.residue = False \n",
    "        if len(batches) % self.n_batches != 0:\n",
    "            self.residue = True\n",
    "        self.index = 0\n",
    "        self.device = device\n",
    "\n",
    "    def _to_tensor(self, datas):\n",
    "        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
    "        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
    "        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n",
    "        return (x, seq_len, mask), y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.residue and self.index == self.n_batches:\n",
    "            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.residue:\n",
    "            return self.n_batches + 1\n",
    "        else:\n",
    "            return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce627d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_iterator(dataset, config):\n",
    "    iter = DatasetIterater(dataset, config.batch_size, config.device)\n",
    "    return iter\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f276a0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 17:10:04.507232: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from transformers import AdamW\n",
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    for name, w in model.named_parameters():\n",
    "        if exclude not in name:\n",
    "            if len(w.size()) < 2:\n",
    "                continue\n",
    "            if 'weight' in name:\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0)\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba755b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, train_iter, dev_iter, test_iter,data_type):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                         lr=config.learning_rate)\n",
    "    total_batch = 0 \n",
    "    last_improve = 0\n",
    "    dev_best_recall = float('-inf')\n",
    "    model.train()\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            model.train()\n",
    "            outputs = model(trains)\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if total_batch % 1000 == 0:\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                dev_acc ,dev_recall,dev_loss = evaluate(config, model, dev_iter,test=False)\n",
    "                if dev_recall > dev_best_recall:\n",
    "                    dev_best_recall = dev_recall\n",
    "                    torch.save(model.state_dict(), os.path.join(save_model_dir, '{}-{}.pth'.format(data_type,total_batch)))\n",
    "                    save_path = config.save_model_dir + '/{}-{}.pth'.format(data_type,total_batch)\n",
    "                    last_improve = total_batch\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {},  Train Loss: {:.4f},  Train Acc: {:.2f} ,dev recall: {:.2f} ,Val Loss: {:.2f},  Val Acc: {:.2f},  Time: {}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_recall, dev_loss, dev_acc, time_dif))\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > config.require_improvement:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "    test(config, model,test_iter,save_path)\n",
    "def test(config, model, test_iter, save_path):\n",
    "    # test\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, recall,test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%},  Test Recall: {2:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc,recall))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f0d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            outputs = model(texts)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    recall = metrics.recall_score(labels_all, predict_all,average='micro')\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, recall ,loss_total / len(data_iter), report, confusion\n",
    "    return acc,recall,loss_total / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b9adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "\n",
    "    \"\"\"配置参数\"\"\"\n",
    "    def __init__(self, dataset, data_type):\n",
    "        self.model_name = 'bert'\n",
    "        self.train_path = dataset + '/train_{}.txt'.format(data_type)                                \n",
    "        self.dev_path =  dataset +  '/dev_{}.txt'.format(data_type)                                      \n",
    "        self.test_path = dataset + '/test_{}.txt'.format(data_type)                                     \n",
    "        if data_type in ['model_3','model_5']:\n",
    "            self.class_list = ['0','1','2']\n",
    "        else:\n",
    "            self.class_list = ['0','1']\n",
    "        self.save_model_dir =   './models/'        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "        self.require_improvement = 1000                                 \n",
    "        self.num_classes = len(self.class_list)                         \n",
    "        self.num_epochs = 5                                             \n",
    "        self.batch_size = 64                                        \n",
    "        self.pad_size = 32                                              \n",
    "        self.learning_rate = 5e-5                                       \n",
    "        self.bert_path = './chinese-roberta-wwm-ext/'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n",
    "        self.hidden_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc4d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time usage: 0:07:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese-roberta-wwm-ext/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/heguxiang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, BertTokenizer\n",
    "dataset = './data'  \n",
    "def run(data_type):\n",
    "    config = Config(dataset,data_type)\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    start_time = time.time()\n",
    "    print(\"Loading data...\")\n",
    "    train_data, dev_data, test_data = build_dataset(config)\n",
    "    train_iter = build_iterator(train_data, config)\n",
    "    dev_iter = build_iterator(dev_data, config)\n",
    "    test_iter = build_iterator(test_data, config)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "    model = Model(config).to(config.device)\n",
    "    train(config, model, train_iter, dev_iter,test_iter,data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ddcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run('model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run('model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76611a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run('model_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "run('model_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89056da",
   "metadata": {},
   "outputs": [],
   "source": [
    "run('model_5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
